METRICS
model evaluation metrics
	confusion matrix: NxN (N is possibilities of target outcome) that shows frequency of true positives, true negatives, false positives, and false negatives)
	f1 score: uses metrics from confusion matrix to make one number
	gain and lift charts
	k-s score
	AUC - ROC: 
	log loss
	gini coefficient
	concordant - discordant ratio
	root mean squared error 
	root mean squared log error
	r-squared
	cross validation
	
ml model evaluation metrics
	eval metric vs loss function. loss is during training, eval is post-training
	supervised learning metrics
		classification
			accuracy: total correct predictions (could be 1 or 0, doesn't matter) / total predictions. may or may not tell us much, depends on how the data looks
			precision (care more about false positives): true postives / count where prediction = 1
			recall (care more about false negatives): true positives / count where actual = 1
			f1 score (very sensitive to what class you label positive): harmonic mean of precision and recall. harmonic takes smaller values more seriously, weights them
			matthew correlation coefficient: looks at all four numbers from confusion matrix
		regression

ROC and AUC
	basically graphing the proportion of correctly-classified samples against the proportion of incorrectly-classified samples. 
	roc: helps identify best threshold for making a decision
	auc: compare the areas under the curve of different models to help you evaluate them
	
Building a Baseline
	baseline: simple model, easy to implement, not complex. you compare more complex builds to this one, to make sure you're actually making measurable improvements for the added time adn complexity
	baselines can often be very effective, if tuned properly. comparing a novel appraoch to a weak baseline give the false impression of improvement/progress. make sure the baseline is rigorously set up to make the testing fair
	improving baselines strategically:
		do not add unecessary complexity: start w simple model that is known to perform reasonably well. if adding complex increases performance marginally, leave the complexity out
		find the limits of your baseline: extensively tune parameters, use regularization techniques, do some data massaging
		gradually progress toward complex models: add one parameter at a time, assess the performance, then decide to keep it or nah. always be aware and be able to express what you are trading off when adding parameters


AUTO ML
	helps non-expert practitioners quickly discover reasonably good models for their classification or regression tasks
	auto ml library: install it and play around.

DATA PARTITIONING
	when data size is insufficient, train-test-split may be inappropriate. k-fold cross validation might be a better move

MODEL GENERALIZATION
	biggest problems are overfitting and underfitting
	underfitting is obvious b/c the model will perform poorly on the test data
	overfitting is the largest problem. You can reduce risk by train/test/split or using resampling: aka training multiple models on different subsets of the data, then aggregating their prediction (averge for regression, majority vote for classification)

HYPERPARAMETERS AND PARAMETERS (DATACAMP)
	parameters: components of the final model that the algo discovers as it learns. simple example: the coefficients of a linear regression model are the model's parameters
	parameters, tree based models: parameters are in the nodes themselves: what feature and what value was split on, for example
	hyperparameters: something you set BEFORE the modeling process. the model doesn't learn or infer these. printing out the instance of our model will show the hyperparameters used
	some hypers won't help model performance at all; they are related to computational decisions or what information to retain for future analysis
		for example, using a random forest model: n_jobs (only changes modeling time), random_state, and verbose. 
	important hyperparameters, using random forest: n_estimators, max_features, max_depth and min_sample_leaf, (maybe) criterion, 
	top tips for deciding which values to try for different hyperparameters
		you can use for loops: given a list of n_estimator possibilites, for example, loop through each and create a model with that hyper parameter, append the accuracy scores of each to a list. then make a dataframe to store the results
		if you need to test a ton of values, then you could use the range() function and graph the results, rather than using a dataframe
		if you need to generate a list of FLOAT values, range() won't work. use numpy's linespace() function instead

GRID SEARCH (DATACAMP)
	grid search is related to automated hyperparameter training. 
	pros: saves lines of code; does away with nested loops for multiple hyperparameters, finds the best model within the grid (make sure you make the possible values reasonable)
	cons: expensive: the number of models created will quickly grow in an exponential way. lastly, it's uninformed, meaning the results of the previous model don't inform how the next model is created. it could be better or worse
	it's a sci-kit learn object: GridSearchCV. amazing stuff. 
	the GridSearchCV object has a ton of useful properties you can explore. super amazing stuff.

RANDOM SEARCH (DATACAMP)
	similiar to grid search
	randomly sample from the n combinations of hyperparameter values and try those.
	why does this work? not all hyperparameters are that important, and using probability, you can reasonably expect to have a 95% chance if you take >= 59 samples
	scikit learn has a RandomSearchCV module!!
	key differences from grid_search: n_iter and param_distribution
	grid v random: exhaustive v sampling, more expensive v less, guaranteed to find best score v not guaranteed

INFORMED SEARCH (DATACAMP)	
	coarse to fine tuning: combines grid search and random search in a cool way. 
	conduct random search, then grid search promising areas of the random search. continue until search space is too small or optimal result is obtained
	informed search, bayesian statistics. what's bayes' rule: calculates the probability of event A given that B has already occured.
	hyperopt: libary for bayesian hyperparamter tuning
	
	
	